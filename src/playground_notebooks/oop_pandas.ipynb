{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'C:/Users/Lenovo/Desktop/exam_eti/containerized_tool/data_analysis_tool/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SCALA_VERSION = '2.12'\n",
    "SPARK_VERSION = '3.1.2'\n",
    "import findspark\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.spark:spark-sql-kafka-0-10_{SCALA_VERSION}:{SPARK_VERSION} pyspark-shell'\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, TimestampType, StructField\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n"
     ]
    }
   ],
   "source": [
    "import mysql_analyzer\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from mysql.connector.errors import Error\n",
    "import dat\n",
    "from faker import Faker\n",
    "import random\n",
    "import json\n",
    "from json import dumps\n",
    "from json import loads\n",
    "import time\n",
    "from datetime import datetime\n",
    "from kafka import KafkaProducer, TopicPartition, KafkaConsumer\n",
    "import xlsxwriter\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark session with gathering stream package and some configs around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master('local[*]')\\\n",
    "        .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "        .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "        .appName(\"myAppName\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating mysql_analyzer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_works = mysql_analyzer.mysql_profiler('localhost',os.environ['MYSQLSERVER_USER'],\n",
    "                os.environ['MYSQLSERVER_PASS'],'sakila')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created CDC table for customer table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql_works.multiple_dataset_apply_mysql_insert(\"CREATE TABLE \\\n",
    "# sakila.users_change_events (log_id BIGINT AUTO_INCREMENT,\\\n",
    "#   event_type      TEXT,\\\n",
    "#   event_timestamp TIMESTAMP,\\\n",
    "#   user_id         INT,\\\n",
    "#   user_name       TEXT,\\\n",
    "#   user_email      TEXT,\\\n",
    "#   PRIMARY KEY (log_id))\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created Trigger that insert CDC transactions into CDC table created on previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql_works.multiple_dataset_apply_mysql_insert(\"CREATE TRIGGER sakila.user_insert_capture AFTER INSERT ON sakila.customer FOR EACH ROW \\\n",
    "#   BEGIN INSERT INTO sakila.users_change_events \\\n",
    "#   (event_type, \\\n",
    "#    event_timestamp, \\\n",
    "#    user_id, \\\n",
    "#    user_name, \\\n",
    "#    user_email) \\\n",
    "#  VALUES ( \\\n",
    "#    'INSERT', \\\n",
    "#    now(), \\\n",
    "#    user_id, \\\n",
    "#    user_name, \\\n",
    "#    user_email); \\\n",
    "#   END;\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created lastest cdc timestamp holding table \n",
    "### We are reading cdc last timestamp from previous data write action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql_works.multiple_dataset_apply_mysql_insert(\"CREATE TABLE \\\n",
    "# sakila.latest_cdc_timestamp (log_id BIGINT AUTO_INCREMENT,\\\n",
    "#   event_type      TEXT,\\\n",
    "#   event_timestamp TIMESTAMP,\\\n",
    "#   PRIMARY KEY (log_id))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql_works.multiple_dataset_apply_mysql_insert(f\"INSERT INTO sakila.latest_cdc_timestamp \\\n",
    "#                                                 (event_type,event_timestamp) \\\n",
    "#                                                 VALUES ('{mysql_works.multiple_dataset_apply_mysql_query \\\n",
    "#                                                  ('SELECT event_type FROM users_change_events ORDER BY event_timestamp DESC LIMIT 1')[0][0]}','{mysql_works.multiple_dataset_apply_mysql_query('SELECT event_timestamp FROM users_change_events ORDER BY event_timestamp DESC LIMIT 1')[0][0]}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka script to write data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producer Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n"
     ]
    }
   ],
   "source": [
    "# Inserting into latest_cdc_timestamp table, latest CDC timestamp from CDC table to be able to detect if any new CDC occurred\n",
    "mysql_works.multiple_dataset_apply_mysql_insert(f\"INSERT INTO sakila.latest_cdc_timestamp \\\n",
    "                                                (event_type,event_timestamp) \\\n",
    "                                                VALUES ('{mysql_works.multiple_dataset_apply_mysql_query('SELECT event_type FROM users_change_events ORDER BY event_timestamp DESC LIMIT 1')[0][0]}','{mysql_works.multiple_dataset_apply_mysql_query('SELECT event_timestamp FROM users_change_events ORDER BY event_timestamp DESC LIMIT 1')[0][0]}')\")\n",
    "# # Creating fake records to create data flow to MYSQL db\n",
    "mysql_works.fake_record_creator_sakila()\n",
    "\n",
    "# Getting CDC timestamps from related table to use on below if statement\n",
    "latest_saved_cdc_log = mysql_works.multiple_dataset_apply_mysql_query('SELECT max(event_timestamp) \\\n",
    "                                                                      FROM latest_cdc_timestamp')[0][0]\n",
    "\n",
    "latest_real_cdc_log = mysql_works.multiple_dataset_apply_mysql_query('SELECT max(event_timestamp) \\\n",
    "                                                                     FROM users_change_events')[0][0]\n",
    "\n",
    "# If CDC occurred instantiating KafkaProducre class and saving data into Kafka topic\n",
    "if latest_saved_cdc_log < latest_real_cdc_log:\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers='settled-terrapin-12518-eu2-kafka.upstash.io:9092',\n",
    "        sasl_mechanism='SCRAM-SHA-256',\n",
    "        security_protocol='SASL_SSL',\n",
    "        sasl_plain_username='c2V0dGxlZC10ZXJyYXBpbi0xMjUxOCTBb5AEffUiTulATzsbFtDRxbvhkO0Wsnc',\n",
    "        sasl_plain_password='N2E2ZGVjY2UtZDY4YS00MjM4LTk5NTktMjU1OTRiZWQ4Y2Ix',\n",
    "        value_serializer = lambda m : dumps(m, default=str).encode(\"utf-8\")\n",
    "        # api_version_auto_timeout_ms=100000,    \n",
    "    )\n",
    "\n",
    "    for record in mysql_works.multiple_dataset_apply_mysql_query(f'SELECT * FROM customer WHERE last_update > \"{latest_saved_cdc_log}\"'):\n",
    "        data_dict = {\"customer_id\" : record[0],\"store_id\" : record[1],\"first_name\" : record[2] \\\n",
    "                     ,\"first_name\" : record[3],\"email\" : record[4],\"address_id\" : record[5] \\\n",
    "                      ,  \"last_update\" : record[8]}\n",
    "        producer.send(\"mysql_write\",data_dict) \n",
    "    producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer Kafka saved data in local xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'mysql_write'\n",
    "tp = TopicPartition(topic,0)\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    bootstrap_servers='settled-terrapin-12518-eu2-kafka.upstash.io:9092',\n",
    "    sasl_mechanism='SCRAM-SHA-256',\n",
    "    security_protocol='SASL_SSL',\n",
    "    sasl_plain_username='c2V0dGxlZC10ZXJyYXBpbi0xMjUxOCTBb5AEffUiTulATzsbFtDRxbvhkO0Wsnc',\n",
    "    sasl_plain_password='N2E2ZGVjY2UtZDY4YS00MjM4LTk5NTktMjU1OTRiZWQ4Y2Ix',\n",
    "    # group_id='bacak',\n",
    "    auto_offset_reset='earliest',\n",
    "    # max_poll_interval_ms=30,\n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "# Configs to get latest offset\n",
    "consumer.assign([tp])\n",
    "\n",
    "# obtain the last offset value\n",
    "consumer.seek_to_end(tp)\n",
    "lastOffset = consumer.position(tp)\n",
    "\n",
    "# Configs to get latest offset\n",
    "consumer.seek_to_beginning(tp)   \n",
    "\n",
    "# Saved topic messages into dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Stopped reading at latest offset\n",
    "for message in consumer:\n",
    "    if message.offset == lastOffset - 1:\n",
    "        break\n",
    "    else:\n",
    "        message = message.value;\n",
    "        # print(message)\n",
    "        # print('{}'.format(message))\n",
    "        df_raw = pd.json_normalize(message, max_level=0)\n",
    "        df = pd.concat([df, df_raw])\n",
    "        # print(df_ult)\n",
    "        # dataframe_dict[]\n",
    "\n",
    "# \n",
    "\n",
    "df.to_csv('stream_df.csv', index=False) \n",
    "# writer = pd.ExcelWriter(\"stream_df.xlsx\", engine=\"xlsxwriter\")\n",
    "# df.to_excel(writer, sheet_name = \"df\")\n",
    "# writer.close()\n",
    "\n",
    "\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read saved xlsx as spark stream source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Source Schema\n",
    "df_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"store_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"address_id\", StringType(), True),\n",
    "    StructField(\"latest_update\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDF = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .schema(df_schema) \\\n",
    "    .csv(\"C:/Users/Lenovo/Desktop/exam_eti/containerized_tool/data_analysis_tool/src/playground_notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- address_id: string (nullable = true)\n",
      " |-- latest_update: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_count = csvDF.groupBy(\"store_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowedCounts = csvDF \\\n",
    "    .withWatermark(\"latest_update\", \"1 minutes\") \\\n",
    "    .groupBy(F.window(csvDF.latest_update, \"30 seconds\"),\n",
    "        csvDF.store_id) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowedCounts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expression = [\"cast(window as string) as window\", \"cast('store_id' as string) as store_id\", \"cast('count' as string) as count\"]\n",
    "# windowedCounts=windowedCounts.selectExpr(expression)\n",
    "# expression = [\"cast(window as binary) as window\", \"cast('store_id' as binary) as store_id\", \"cast('count' as binary) as count\"]\n",
    "# windowedCounts=windowedCounts.selectExpr(expression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowedCounts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windowedCounts.writeStream \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .format(\"json\").option(\"path\", \"C:/Users/Lenovo/Desktop/exam_eti/containerized_tool/data_analysis_tool/src/playground_notebooks/csv_sink\") \\\n",
    "#     .option(\"checkpointLocation\", \"C:/Users/Lenovo/Desktop/exam_eti/containerized_tool/data_analysis_tool/src/playground_notebooks/csv_sink\") \\\n",
    "#     .start()\n",
    "\n",
    "# windowedCounts.writeStream\\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .format(\"console\").start()\n",
    "\n",
    "# danyal = windowedCounts.writeStream.format(\"kafka\").outputMode(\"complete\")  \\\n",
    "#     .option(\"kafka.bootstrap.servers\",'settled-terrapin-12518-eu2-kafka.upstash.io:9092')\\\n",
    "#     .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-256\")\\\n",
    "#     .option(\"kafka.security.protocol\", \"SASL_SSL\")\\\n",
    "#     .option(\"kafka.sasl.jaas.config\",\"\"\"org.apache.kafka.common.security.plain.PlainLoginModule required username=\"c2V0dGxlZC10ZXJyYXBpbi0xMjUxOCSqaSFgt-fI-8JyIV50sk_wCOG7dRr8LsY\" password=\"Y2FhZGE3ZWQtYzQxOC00ZTdiLWJlZjUtOGRhMjJjN2YwZjU1\";\"\"\")\\\n",
    "#     .option(\"topic\", \"aggregated_store_id\") \\\n",
    "#     .option(\"checkpointLocation\", \"C:/Users/Lenovo/Desktop/exam_eti/containerized_tool/data_analysis_tool/src/playground_notebooks/csv_sink\") \\\n",
    "#     .start()\n",
    "\n",
    "# danyal.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x18efc2b0590>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windowedCounts \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"didik\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+\n",
      "|              window|store_id|count|\n",
      "+--------------------+--------+-----+\n",
      "|{2024-06-28 15:39...|       1|    4|\n",
      "|{2024-06-28 15:14...|       1|    4|\n",
      "|{2024-06-28 15:11...|       1|    5|\n",
      "|{2024-06-28 15:15...|       1|    4|\n",
      "|{2024-06-28 15:38...|       1|    4|\n",
      "|{2024-06-28 15:39...|       1|    4|\n",
      "|{2024-06-28 15:19...|       1|    5|\n",
      "|{2024-06-28 16:10...|       2|    2|\n",
      "|{2024-06-28 15:43...|       1|    5|\n",
      "|{2024-06-28 14:34...|       1|    2|\n",
      "|{2024-06-28 15:43...|       1|    4|\n",
      "|{2024-06-28 15:20...|       1|    4|\n",
      "|{2024-06-28 15:19...|       1|    5|\n",
      "|{2024-06-28 14:06...|       1|    4|\n",
      "|{2024-06-28 15:36...|       1|    4|\n",
      "|{2024-06-28 16:10...|       1|    2|\n",
      "|{2024-06-28 15:34...|       1|    3|\n",
      "|{2024-06-28 15:14...|       1|    4|\n",
      "|{2024-06-28 15:37...|       1|    5|\n",
      "|{2024-06-28 15:13...|       1|    3|\n",
      "+--------------------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from didik\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous works on below chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql_works.multiple_dataset_apply_containing_cols_mysql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql_works.find_value_mysql('Hillyer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql_works.multiple_dataset_apply_mysql_query('SELECT * FROM criket_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql_works.multiple_dataset_apply_mysql_query('SELECT team, COUNT(team) as game_played,\\\n",
    "#                                           CAST(SUM(CASE WHEN team=WonBy THEN 1 ELSE 0 END)AS SIGNED) AS game_won FROM\\\n",
    "#                                          (SELECT TeamA AS team, WonBy from criket_table\\\n",
    "#                                          UNION ALL\\\n",
    "#                                          SELECT TeamB AS team, WonBy from criket_table) AS sub\\\n",
    "#                                          GROUP BY team\\\n",
    "#                                          ORDER BY game_won'\n",
    "#                                          )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
