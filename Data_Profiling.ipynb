{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "#import pyspark\n",
    "\n",
    "\n",
    "# use glob to get all the csv files \n",
    "# in the folder\n",
    "path = os.getcwd()\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "#xlsx location\n",
    "xlsx_files = glob.glob(os.path.join(path, \"*.xlsx\"))\n",
    "\n",
    "\n",
    "#loop over the list of csv files\n",
    "for f in csv_files:\n",
    "      \n",
    "    # read the csv file\n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "      \n",
    "    # print the location and filename\n",
    "    #print('Location:', f)\n",
    "    print('File Name:', f.split(\"\\\\\")[-1])\n",
    "      \n",
    "    # print the content\n",
    "    # print('Content:')\n",
    "    # df.apply(str)\n",
    "    # display(df.head())\n",
    "    for (columnName, columnData) in df.items():\n",
    "        # if \"time\" in columnName.lower() or \"date\" in columnName.lower():\n",
    "        #     #print(columnName)\n",
    "        #     df[columnName] = df[columnName].apply(lambda x: str(x))\n",
    "        df[columnName] = df[columnName].apply(lambda x: str(x))\n",
    "        # nan_ds = len(df[columnName] == 'nan')\n",
    "        # df[columnName]=df[columnName].values.astype('string')\n",
    "        # df.astype({'columnName': str})\n",
    "        #pd.to_datetime(df[columnName])\n",
    "    # display(df.convert_dtypes(convert_string=True,convert_integer=True))\n",
    "    #display(df.dtypes)\n",
    "    # pd.DataFrame(df.isna().sum(),columns=['column_name','null_count'])\n",
    "    null_ds = pd.DataFrame(df.isna().sum(),columns=['Null_Count'])\n",
    "    # nan_ds =df.size()['nan']\n",
    "    # nan_ds = pd.DataFrame(df.value_counts()['nan'],columns=['Nan_Count'])\n",
    "    nan_ds = pd.DataFrame(df.isin(['nan','null','Nan', 'NaN', 'Null', ' ', '']).sum(),columns=['Nan&Null_Count']) \n",
    "    # display(null_ds)\n",
    "    # display(nan_ds)\n",
    "    # display(df.describe(include='all'))\n",
    "    # Example 6: Get count duplicate rows\n",
    "    number_duplication = len(df)-len(df.drop_duplicates())\n",
    "    dict_d = {'col1': number_duplication}\n",
    "    df2 = pd.DataFrame(data=dict_d,index=[\"count_dup\"])\n",
    "    # display(df2)\n",
    "\n",
    "\n",
    "\n",
    "    # # value_counts = pd.DataFrame(np.unique(df.values, return_counts=True))\n",
    "    # # display(value_counts)\n",
    "\n",
    "    with pd.ExcelWriter(f'{f}.xlsx') as writer:  \n",
    "        df.head().to_excel(writer, sheet_name='Head_5_Rows')\n",
    "        # null_ds.to_excel(writer, sheet_name='Null_Count_Columns')\n",
    "        nan_ds.to_excel(writer, sheet_name='Null_Count_Columns')\n",
    "        df.describe(include='all').to_excel(writer, sheet_name='Stats_of_columns')\n",
    "        df2.to_excel(writer, sheet_name='Duplicate_Row_Count')\n",
    "    #print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# loop over the list of xlsx files\n",
    "for f in xlsx_files:\n",
    "\n",
    "    xls = pd.ExcelFile(f)\n",
    "\n",
    "    # Now you can list all sheets in the file\n",
    "    xls.sheet_names\n",
    "\n",
    "    sheet_to_df_map = {}\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        sheet_to_df_map[sheet_name] = xls.parse(sheet_name)\n",
    "\n",
    "    \n",
    "\n",
    "    # df_num = \"0\"\n",
    "    # read the csv file\n",
    "    for key, value in sheet_to_df_map.items():\n",
    "        # display(value)\n",
    "        # null_ds = pd.DataFrame(value.isna().sum(),columns=['Null_Count'])\n",
    "        # display(null_ds)\n",
    "        value = value.astype(str)\n",
    "        # value = value.apply(lambda x: str(x))\n",
    "        nan_ds = pd.DataFrame(value.isin(['nan','null','Nan', 'NaN', 'Null', ' ', '']).sum(),columns=['Nan&Null_Count']) \n",
    "        number_duplication = len(value)-len(value.drop_duplicates())\n",
    "        dict_d = {'col1': number_duplication}\n",
    "        df2 = pd.DataFrame(data=dict_d,index=[\"count_dup\"])\n",
    "        with pd.ExcelWriter(f'{f}_{key}.xlsx') as writer:  \n",
    "            value.head().to_excel(writer, sheet_name='Head_5_Rows')\n",
    "            # null_ds.to_excel(writer, sheet_name='Null_Count_Columns')\n",
    "            nan_ds.to_excel(writer, sheet_name='Null_Count_Columns')\n",
    "            value.describe(include='all').to_excel(writer, sheet_name='Stats_of_columns')\n",
    "            df2.to_excel(writer, sheet_name='Duplicate_Row_Count')\n",
    "\n",
    "\n",
    "\n",
    "        # print(value)\n",
    "        # pd.read_excel(f, sheet_name=\"key\")\n",
    "        # df_num += 1\n",
    "    # display(df_num)  \n",
    "    # # print the location and filename\n",
    "    # #print('Location:', f)\n",
    "    # print('File Name:', f.split(\"\\\\\")[-1])\n",
    "      \n",
    "    # # print the content\n",
    "    # #print('Content:')\n",
    "    # display(df_2.head())\n",
    "    # #print()\n",
    "\n",
    "    # for (columnName, columnData) in df_2.items():\n",
    "    #     df_2[columnName] = df_2[columnName].apply(lambda x: str(x))\n",
    "    # display(df_2.dtypes)\n",
    "    \n",
    "\n",
    "    # null_ds = pd.DataFrame(df_2.isna().sum(),columns=['Null_Count'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kolonlara gore bos ve null degerlerin adedini grupladim asagida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: APSO Failure tasks sample.csv\n",
      "File Name: APSO Failures sample.csv\n",
      "BATCH True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cihan_Giray_Oner\\AppData\\Local\\Temp\\ipykernel_26396\\747980920.py:67: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  e = e.append(df_12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACHINE True\n",
      "PRODUCT True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cihan_Giray_Oner\\AppData\\Local\\Temp\\ipykernel_26396\\747980920.py:67: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  e = e.append(df_12)\n",
      "C:\\Users\\Cihan_Giray_Oner\\AppData\\Local\\Temp\\ipykernel_26396\\747980920.py:67: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  e = e.append(df_12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: Batch all.csv\n",
      "File Name: Batch Genealogy sample.csv\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Cihan_Giray_Oner\\Desktop\\Technical_Documentation\\Projects\\TEVA\\Data Samples\\PANDAS_data_frame\\Data_Profiling.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cihan_Giray_Oner/Desktop/Technical_Documentation/Projects/TEVA/Data%20Samples/PANDAS_data_frame/Data_Profiling.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#loop over the list of csv files\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cihan_Giray_Oner/Desktop/Technical_Documentation/Projects/TEVA/Data%20Samples/PANDAS_data_frame/Data_Profiling.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m csv_files:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cihan_Giray_Oner/Desktop/Technical_Documentation/Projects/TEVA/Data%20Samples/PANDAS_data_frame/Data_Profiling.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m       \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cihan_Giray_Oner/Desktop/Technical_Documentation/Projects/TEVA/Data%20Samples/PANDAS_data_frame/Data_Profiling.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# read the csv file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cihan_Giray_Oner/Desktop/Technical_Documentation/Projects/TEVA/Data%20Samples/PANDAS_data_frame/Data_Profiling.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# df = pd.read_csv(f, low_memory=False,keep_default_na = False, na_values=\"Null\")\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Cihan_Giray_Oner/Desktop/Technical_Documentation/Projects/TEVA/Data%20Samples/PANDAS_data_frame/Data_Profiling.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(f, low_memory\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cihan_Giray_Oner/Desktop/Technical_Documentation/Projects/TEVA/Data%20Samples/PANDAS_data_frame/Data_Profiling.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# print the location and filename\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cihan_Giray_Oner/Desktop/Technical_Documentation/Projects/TEVA/Data%20Samples/PANDAS_data_frame/Data_Profiling.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m#print('Location:', f)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Cihan_Giray_Oner/Desktop/Technical_Documentation/Projects/TEVA/Data%20Samples/PANDAS_data_frame/Data_Profiling.ipynb#W2sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFile Name:\u001b[39m\u001b[39m'\u001b[39m, f\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Cihan_Giray_Oner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Cihan_Giray_Oner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Cihan_Giray_Oner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Cihan_Giray_Oner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\Cihan_Giray_Oner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Cihan_Giray_Oner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:235\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[0;32m    234\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread(nrows)\n\u001b[0;32m    236\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_first_chunk:\n",
      "File \u001b[1;32mc:\\Users\\Cihan_Giray_Oner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:790\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Cihan_Giray_Oner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:883\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Cihan_Giray_Oner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "#import pyspark\n",
    "\n",
    "\n",
    "# use glob to get all the csv files \n",
    "# in the folder\n",
    "path = os.getcwd()\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "#xlsx location\n",
    "xlsx_files = glob.glob(os.path.join(path, \"*.xlsx\"))\n",
    "\n",
    "\n",
    "#loop over the list of csv files\n",
    "for f in csv_files:\n",
    "      \n",
    "    # read the csv file\n",
    "    # df = pd.read_csv(f, low_memory=False,keep_default_na = False, na_values=\"Null\")\n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "\n",
    "      \n",
    "    # print the location and filename\n",
    "    #print('Location:', f)\n",
    "    print('File Name:', f.split(\"\\\\\")[-1])\n",
    "      \n",
    "    # print the content\n",
    "    # print('Content:')\n",
    "    # df.apply(str)\n",
    "    # display(df.head())\n",
    "    # for (columnName, columnData) in df.items():\n",
    "    #     # if \"time\" in columnName.lower() or \"date\" in columnName.lower():\n",
    "    #     #     #print(columnName)\n",
    "    #     #     df[columnName] = df[columnName].apply(lambda x: str(x))\n",
    "    #     df[columnName] = df[columnName].apply(lambda x: str(x))\n",
    "    #     nan_ds = len(df[columnName] == 'nan')\n",
    "    #     df[columnName]=df[columnName].values.astype(str)\n",
    "    #     df.astype({'columnName': str})\n",
    "    #     pd.to_datetime(df[columnName])\n",
    "    # display(df.convert_dtypes(convert_string=True,convert_integer=True))\n",
    "    # display(df.dtypes)\n",
    "    # pd.DataFrame(df.isna().sum(),columns=['column_name','null_count'])\n",
    "    # null_ds = pd.DataFrame(df.isna().sum(),columns=['Null_Count'])\n",
    "    # nan_ds =df.size()['nan']\n",
    "    # nan_ds = pd.DataFrame(df.value_counts()['nan'],columns=['Nan_Count'])\n",
    "    # nan_ds = pd.DataFrame(df.isin(['Null', ' ', '']).sum(),columns=['Nan&Null_Count']) \n",
    "    # # display(null_ds)\n",
    "    # # display(nan_ds)\n",
    "    # # display(df.describe(include='all'))\n",
    "    # # Example 6: Get count duplicate rows\n",
    "    # number_duplication = len(df)-len(df.drop_duplicates())\n",
    "    # dict_d = {'col1': number_duplication}\n",
    "    # df2 = pd.DataFrame(data=dict_d,index=[\"count_dup\"])\n",
    "    e = pd.DataFrame()\n",
    "    for (columnName, columnData) in df.items():\n",
    "        df[columnName] = df[columnName].apply(lambda x: str(x))\n",
    "        # if columnData.notnull().count() > 1:\n",
    "        if columnData.notnull().all() == False and columnData.isin([' ','']).any() == True:\n",
    "            print(columnName,\"True\")\n",
    "            df_12 = pd.DataFrame(df[columnName].value_counts(dropna=False))\n",
    "            df_12 = df_12.loc[pd.Index([\" \", \"nan\"])]\n",
    "            # df_12 = pd.DataFrame(df[columnName].value_counts()[' ','','nan'])\n",
    "            e = e.append(df_12)\n",
    "            # df4 = pd.concat(df_12,axis=1)\n",
    "            # print(df[columnName].value_counts())\n",
    "            # df4 = df.apply(pd.value_counts)\n",
    "            with pd.ExcelWriter(f'{f}.xlsx') as writer:\n",
    "                e.to_excel(writer, sheet_name='Value_freq')\n",
    "\n",
    "\n",
    "\n",
    "    # # value_counts = pd.DataFrame(np.unique(df.values, return_counts=True))\n",
    "    # # display(value_counts)\n",
    "\n",
    "    # with pd.ExcelWriter(f'{f}.xlsx') as writer:  \n",
    "    #     # df.head().to_excel(writer, sheet_name='Head_5_Rows')\n",
    "    #     # # null_ds.to_excel(writer, sheet_name='Null_Count_Columns')\n",
    "    #     # nan_ds.to_excel(writer, sheet_name='Null_Count_Columns')\n",
    "    #     # df.describe(include='all').to_excel(writer, sheet_name='Stats_of_columns')\n",
    "    #     # df2.to_excel(writer, sheet_name='Duplicate_Row_Count')\n",
    "    #     df4.to_excel(writer, sheet_name='Value_freq')\n",
    "    #print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# loop over the list of xlsx files\n",
    "# for f in xlsx_files:\n",
    "\n",
    "#     xls = pd.ExcelFile(f)\n",
    "\n",
    "#     # read_file = pd.read_excel (f)\n",
    "\n",
    "\n",
    "\n",
    "#     # Now you can list all sheets in the file\n",
    "#     xls.sheet_names\n",
    "\n",
    "#     sheet_to_df_map = {}\n",
    "#     for sheet_name in xls.sheet_names:\n",
    "#         sheet_to_df_map[sheet_name] = xls.parse(sheet_name)\n",
    "\n",
    "    \n",
    "# #######converted all tabs to csv\n",
    "#     # df_num = \"0\"\n",
    "#     # read the csv file\n",
    "#     for key, value in sheet_to_df_map.items():\n",
    "#         value.to_csv (f'{f}_{key}.csv', \n",
    "#                   index = None,\n",
    "#                   header=True)\n",
    "        \n",
    "\n",
    "        # display(value)\n",
    "        # null_ds = pd.DataFrame(value.isna().sum(),columns=['Null_Count'])\n",
    "        # display(null_ds)\n",
    "        # value = value.astype(str)\n",
    "        # # value = value.apply(lambda x: str(x))\n",
    "        # nan_ds = pd.DataFrame(value.isin(['nan','null','Nan', 'NaN', 'Null', ' ', '']).sum(),columns=['Nan&Null_Count']) \n",
    "        # number_duplication = len(value)-len(value.drop_duplicates())\n",
    "        # dict_d = {'col1': number_duplication}\n",
    "        # df2 = pd.DataFrame(data=dict_d,index=[\"count_dup\"])\n",
    "        # value = pd.DataFrame(value)\n",
    "        # e = pd.DataFrame()\n",
    "        # if value.notnull().all() == False and value.isin([' ','']).any() == True:\n",
    "        #     print(key,\"True\")\n",
    "        #     df_13 = pd.DataFrame(value[key].value_counts(dropna=False))\n",
    "        #     df_13 = df_13.loc[pd.Index([\" \", \"nan\"])]\n",
    "        #     # df_12 = pd.DataFrame(df[columnName].value_counts()[' ','','nan'])\n",
    "        #     e = e.append(df_13)\n",
    "        # with pd.ExcelWriter(f'{f}.xlsx') as writer:\n",
    "        #     e.to_excel(writer, sheet_name='Value_freq')\n",
    "        # with pd.ExcelWriter(f'{f}_{key}.xlsx') as writer:\n",
    "            # value.head().to_excel(writer, sheet_name='Head_5_Rows')\n",
    "            # # null_ds.to_excel(writer, sheet_name='Null_Count_Columns')\n",
    "            # nan_ds.to_excel(writer, sheet_name='Null_Count_Columns')\n",
    "            # value.describe(include='all').to_excel(writer, sheet_name='Stats_of_columns')\n",
    "            # df2.to_excel(writer, sheet_name='Duplicate_Row_Count')\n",
    "        # value.to_csv('C:/Users/Cihan_Giray_Oner/Desktop/Technical_Documentation/Projects/TEVA/Data Samples/PANDAS_data_frame', index=False, header=True)\n",
    "\n",
    "\n",
    "        # with open(f'{f}_{key}.csv', 'w', encoding='UTF8') as r:\n",
    "        #     writer = csv.writer(r)\n",
    "\n",
    "        #     writer.writerows(value)\n",
    "\n",
    "    # # write the header\n",
    "    #         writer.writerow(header)\n",
    "\n",
    "#         # print(value)\n",
    "#         # pd.read_excel(f, sheet_name=\"key\")\n",
    "#         # df_num += 1\n",
    "#     # display(df_num)  \n",
    "#     # # print the location and filename\n",
    "#     # #print('Location:', f)\n",
    "#     # print('File Name:', f.split(\"\\\\\")[-1])\n",
    "      \n",
    "#     # # print the content\n",
    "#     # #print('Content:')\n",
    "#     # display(df_2.head())\n",
    "#     # #print()\n",
    "\n",
    "#     # for (columnName, columnData) in df_2.items():\n",
    "#     #     df_2[columnName] = df_2[columnName].apply(lambda x: str(x))\n",
    "#     # display(df_2.dtypes)\n",
    "    \n",
    "\n",
    "#     # null_ds = pd.DataFrame(df_2.isna().sum(),columns=['Null_Count'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection columns holding both NULL and Empty Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: APSO Failure tasks sample.csv\n",
      "File Name: APSO Failures sample.csv\n",
      "File Name: Batch all.csv\n",
      "File Name: Batch Genealogy sample.csv\n",
      "File Name: EQM LOG sample.csv\n",
      "File Name: Inspection lot sample.csv\n",
      "File Name: LIMS sample Apr 2023.csv\n",
      "File Name: Material master all.csv\n",
      "File Name: MES activities from 01_08_2023.csv\n",
      "File Name: MES_Activities.csv\n",
      "File Name: OEE_Order_dim.csv\n",
      "File Name: OEE_Order_fact.csv\n",
      "File Name: OEE_productionline.csv\n",
      "File Name: OEE_Shift_dim.csv\n",
      "File Name: OEE_Shift_fact.csv\n",
      "File Name: TW text all by July 2023.csv\n",
      "File Name: ZE notifications.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "# use glob to get all the csv files \n",
    "# in the folder\n",
    "path = os.getcwd()\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "#xlsx location\n",
    "xlsx_files = glob.glob(os.path.join(path, \"*.xlsx\"))\n",
    "\n",
    "\n",
    "#loop over the list of csv files\n",
    "for f in csv_files:\n",
    "      \n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "\n",
    "    print('File Name:', f.split(\"\\\\\")[-1])\n",
    "      \n",
    "    e = pd.DataFrame()\n",
    "    for (columnName, columnData) in df.items():\n",
    "        df[columnName] = df[columnName].apply(lambda x: str(x))\n",
    "        if columnData.notnull().all() == False and columnData.isin(['']).any() == True:\n",
    "            print(columnName,\"True\")\n",
    "            df_12 = pd.DataFrame(df[columnName].value_counts(dropna=False))\n",
    "            df_12 = df_12.loc[pd.Index([\" \", \"nan\"])]\n",
    "            e = e.append(df_12)\n",
    "            with pd.ExcelWriter(f'{f}.xlsx') as writer:\n",
    "                e.to_excel(writer, sheet_name='Value_freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Birini kapsayan kolonlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDKEY IDKEY\n",
      "STATUS STATUS\n",
      "STATUSDESC STATUSDESC\n",
      "IDORDER IDORDER\n",
      "PERSONNUM PERSONNUM\n",
      "FAILURE_DATE_OCCURENCE FAILURE_DATE_OCCURENCE\n",
      "FAILURE_DESCRIPTIONS FAILURE_DESCRIPTIONS\n",
      "QA_APPROVAL_DATE DATEREC\n",
      "TAKEN_BY_PROD_DATE DATEREC\n",
      "TAKEN_BY_PROD_DATE FAILURE_DATE_OCCURENCE\n",
      "count IDORDER\n",
      "EQMCLASS_eqm_log IDORDER\n",
      "EQMCLASS_mdeqm IDORDER\n",
      "NEGATIVEWEIGHINGPOSSIBLE_mdeqm FIX_ID\n",
      "ALLEQUIPMENTPARAMETRIZED_mdeqmtype IDORDER\n",
      "ALLEQUIPMENTPARAMETRIZED_mdeqmtype FIX_ID\n",
      "APPLYCLEANINGRULETARGETBATCH_mdeqmtype FIX_ID\n",
      "EBRIDENTIFICATIONALLOWED_mdeqmtype FIX_ID\n",
      "other_posted_quantities_to_lot_stock FIX_ID\n",
      "quantity_posted_to_another_material FIX_ID\n",
      "LOT_ORDER_DISPOSITION IDORDER\n",
      "SPEC_ANALYSIS_VERSION IDORDER\n",
      "SPEC_COMPONENT_ORDER IDORDER\n",
      "single_material IDORDER\n",
      "source IDORDER\n",
      "status IDORDER\n",
      "cc_reasoncode_duration_adjustment FIX_ID\n",
      "cc_reasoncode_duration_plannedshutdown FIX_ID\n",
      "availability_warning_threshold FIX_ID\n",
      "production_rate_alarm_threshold FIX_ID\n",
      "production_rate_warning_threshold FIX_ID\n",
      "site_id IDORDER\n",
      "shift_type_id IDORDER\n",
      "notification_item_step IDORDER\n"
     ]
    }
   ],
   "source": [
    "# use glob to get all the csv files \n",
    "# in the folder\n",
    "path = os.getcwd()\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "#xlsx location\n",
    "xlsx_files = glob.glob(os.path.join(path, \"*.xlsx\"))\n",
    "\n",
    "# loop over the list of csv files\n",
    "e = pd.DataFrame()\n",
    "for f in csv_files:\n",
    "      \n",
    "    # read the csv file\n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "    e = pd.concat([e,df])\n",
    "\n",
    "    File_name =  f.split(\"\\\\\")[-1]\n",
    "# d = []\n",
    "# counter = 0\n",
    "for (columnName, columnData) in e.items():\n",
    "    for i in range(len(columnName)):\n",
    "    # e.get('columnName')\n",
    "    # print(e.iloc[:,0])\n",
    "    # print(e.iloc[:,counter])\n",
    "    # print(e.iloc[:,counter].isin(columnData).all())\n",
    "        # if columnName == e.iloc[:,i].name:\n",
    "        #     print(columnName,e.iloc[:,i].name)\n",
    "        if columnData.isin(e.iloc[:,i]).all()  == True and columnData.dropna().empty == False:\n",
    "            print(columnName,e.iloc[:,i].name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAML Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.withColumn(b, regexp_replace(OPERATION, Tabletting (OEE) , Tabletting Balance Tableting\n",
      "0\n",
      "df.withColumn(b, regexp_replace(OPERATION, Tabletting (OEE) , Tabletting Balance Tableting\n",
      "1\n",
      "df.withColumn(b, regexp_replace(OPERATION, Tabletting (OEE) , Tabletting Balance Tableting\n",
      "2\n",
      "df.withColumn(b, regexp_replace(OPERATION, Tabletting (OEE) , Tabletting Balance Tableting\n",
      "3\n",
      "df.withColumn(b, regexp_replace(OPERATION, Tabletting (OEE) , Tabletting Balance Tableting\n",
      "4\n",
      "df.withColumn(b, regexp_replace(OPERATION, Tabletting (OEE) , Tabletting Balance Tableting\n",
      "5\n",
      "df.withColumn(b, regexp_replace(OPERATION, Tabletting (OEE) , Tabletting Balance Tableting\n",
      "6\n",
      "df.withColumn(b, regexp_replace(OPERATION, Tabletting (OEE) , Tabletting Balance Tableting\n",
      "7\n",
      "df.withColumn(b, regexp_replace(OPERATION, Tabletting (OEE) , Tabletting Balance Tableting\n",
      "8\n",
      "df.withColumn(b, regexp_replace(OPERATION, Tabletting (OEE) , Tabletting Balance Tableting\n",
      "9\n",
      "df.withColumn(b, regexp_replace(OPERATION, Tabletting (OEE) , Tabletting Balance Tableting\n",
      "10\n",
      "df.withColumn(b, regexp_replace(OPERATION, Tabletting (OEE) , Tabletting Balance Tableting\n",
      "11\n",
      "df.withColumn(b, regexp_replace(OPERATION, Tabletting (OEE) , Tabletting Balance Tableting\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# import pyyaml as yaml\n",
    "# %pip install PyYAML\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "path = os.getcwd()\n",
    "yaml_files = glob.glob(os.path.join(path, \"*.yaml\"))\n",
    "\n",
    "# for f in yaml_files:\n",
    "    \n",
    "\n",
    "# Read YAML file\n",
    "with open(\"new_1.yaml\", 'r') as stream:\n",
    "    data_loaded = yaml.safe_load(stream)\n",
    "\n",
    "values_map = data_loaded[\"map\"].split(\":\")\n",
    "\n",
    "b = \"OPERATION\"\n",
    "\n",
    "for i in range(len(values_map)):\n",
    "    print(f\"df.withColumn(b, regexp_replace({b}, {values_map[0]}, {values_map[1]}\")\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Catalog Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "#import pyspark\n",
    "\n",
    "\n",
    "# use glob to get all the csv files \n",
    "# in the folder\n",
    "path = os.getcwd()\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "#xlsx location\n",
    "xlsx_files = glob.glob(os.path.join(path, \"*.xlsx\"))\n",
    "\n",
    "\n",
    "#loop over the list of csv files\n",
    "for f in csv_files:\n",
    "\n",
    "    File_name =  f.split(\"\\\\\")[-1]\n",
    "      \n",
    "    # read the csv file\n",
    "    # df = pd.read_csv(f, low_memory=False,keep_default_na = False, na_values=\"Null\")\n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "\n",
    "    for column in df.columns:\n",
    "        print(File_name,column,pd.api.types.infer_dtype(df[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Column Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APSO Failure tasks sample.csv IDKEY 322\n",
      "Inspection lot sample.csv inspection_lot_number 2946\n",
      "Inspection lot sample.csv delivery 2946\n",
      "LIMS sample Apr 2023.csv TEST_RESULT_NUMBER 169941\n",
      "Material master all.csv material_code 8428\n",
      "OEE_Order_dim.csv actual_start 1490\n",
      "OEE_Order_dim.csv cc_actual_start 1490\n",
      "OEE_Order_dim.csv cc_last_actual_start 1490\n",
      "OEE_Order_dim.csv order_id 1490\n",
      "OEE_Order_dim.csv last_actual_start 1490\n",
      "OEE_Order_dim.csv shopfloor_order_id 1490\n",
      "OEE_Order_fact.csv cc_end_time 1161\n",
      "OEE_Order_fact.csv cc_start_time 1161\n",
      "OEE_Order_fact.csv end_time 1161\n",
      "OEE_Order_fact.csv order_fact_id 1161\n",
      "OEE_Order_fact.csv start_time 1161\n",
      "OEE_productionline.csv id 42\n",
      "OEE_productionline.csv name 42\n",
      "OEE_productionline.csv production_unit_description 42\n",
      "OEE_productionline.csv production_unit_id 42\n",
      "OEE_productionline.csv production_unit_name 42\n",
      "OEE_Shift_dim.csv shift_id 7308\n",
      "OEE_Shift_fact.csv shift_fact_id 7275\n",
      "OEE_Shift_fact.csv shift_id 7275\n",
      "OEE_Shift_fact.csv shift_time_account_id 7275\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np# use glob to get all the csv files \n",
    "# in the folder\n",
    "path = os.getcwd()\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "#xlsx location\n",
    "xlsx_files = glob.glob(os.path.join(path, \"*.xlsx\"))\n",
    "\n",
    "# loop over the list of csv files\n",
    "e = pd.DataFrame()\n",
    "for f in csv_files:\n",
    "      \n",
    "    # read the csv file\n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "\n",
    "    File_name =  f.split(\"\\\\\")[-1]\n",
    "# d = []\n",
    "# counter = 0\n",
    "    for (columnName, columnData) in df.items():\n",
    "    #for i in range(len(columnName)):\n",
    "    # e.get('columnName')\n",
    "    # print(e.iloc[:,0])\n",
    "    # print(e.iloc[:,counter])\n",
    "    # print(e.iloc[:,counter].isin(columnData).all())\n",
    "        # if columnName == e.iloc[:,i].name:\n",
    "        #     print(columnName,e.iloc[:,i].name)\n",
    "\n",
    "\n",
    "        #### COUNTS COLUMN LEN WITH NULLS SO RESULTS TRUE FOR COLUMNS WITHOUT NULLS\n",
    "        # if len(columnData)  == columnData.nunique() or columnData.notnull().any() == True: \n",
    "\n",
    "\n",
    "        #### COUNTS WITHOUT NULL VALUES SO RESULTS ARE TRUE FOR ALL COLUMNS\n",
    "        if (len(columnData) - (columnData.isna().sum())) == columnData.nunique() and columnData.notnull().any() == True:\n",
    "            # and columnData.dropna().empty == False:\n",
    "            print(File_name,columnName,len(columnData))\n",
    "        # elif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Spesific value in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# use glob to get all the csv files \n",
    "# in the folder\n",
    "path = os.getcwd()\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "#xlsx location\n",
    "xlsx_files = glob.glob(os.path.join(path, \"*.xlsx\"))\n",
    "\n",
    "# loop over the list of csv files\n",
    "\n",
    "for f in csv_files:\n",
    "      \n",
    "    # read the csv file\n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "    df = df.dropna(how='all',axis='columns')\n",
    "\n",
    "    File_name =  f.split(\"\\\\\")[-1]\n",
    "\n",
    "\n",
    "    for (columnName, columnData) in df.items():\n",
    "        if columnData.isin([\"1443\"]).any() == True:\n",
    "            print(File_name,columnName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Counts by Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: APSO Failure tasks sample.csv\n",
      "File Name: APSO Failures sample.csv\n",
      "File Name: Batch all.csv\n",
      "File Name: Batch Genealogy sample.csv\n",
      "File Name: EQM LOG sample.csv\n",
      "File Name: Inspection lot sample.csv\n",
      "File Name: LIMS sample Apr 2023.csv\n",
      "File Name: Material master all.csv\n",
      "File Name: MES activities from 01_08_2023.csv\n",
      "File Name: OEE_Order_dim.csv\n",
      "File Name: OEE_Order_fact.csv\n",
      "File Name: OEE_productionline.csv\n",
      "File Name: OEE_Shift_dim.csv\n",
      "File Name: OEE_Shift_fact.csv\n",
      "File Name: TW text all by July 2023.csv\n",
      "File Name: ZE notifications.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "#import pyspark\n",
    "\n",
    "\n",
    "# use glob to get all the csv files \n",
    "# in the folder\n",
    "path = os.getcwd()\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "#xlsx location\n",
    "xlsx_files = glob.glob(os.path.join(path, \"*.xlsx\"))\n",
    "\n",
    "\n",
    "#loop over the list of csv files\n",
    "for f in csv_files:\n",
    "      \n",
    "    # read the csv file\n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "      \n",
    "    # print the location and filename\n",
    "    #print('Location:', f)\n",
    "    print('File Name:', f.split(\"\\\\\")[-1])\n",
    "      \n",
    "    # print the content\n",
    "    # print('Content:')\n",
    "    # df.apply(str)\n",
    "    # display(df.head())\n",
    "    # value_counts = df.value_counts()\n",
    "    e = pd.DataFrame()\n",
    "    for (columnName, columnData) in df.items():\n",
    "        # if \"time\" in columnName.lower() or \"date\" in columnName.lower():\n",
    "        #     #print(columnName)\n",
    "        #     df[columnName] = df[columnName].apply(lambda x: str(x))\n",
    "        df[columnName] = df[columnName].apply(lambda x: str(x))\n",
    "        value_counts = df[columnName].value_counts()\n",
    "        # nan_ds = len(df[columnName] == 'nan')\n",
    "        # df[columnName]=df[columnName].values.astype('string')\n",
    "        # df.astype({'columnName': str})\n",
    "        #pd.to_datetime(df[columnName])\n",
    "        value_counts_df = pd.DataFrame(value_counts)\n",
    "        e = pd.concat([e,value_counts_df])\n",
    "        # e.set_index('column_name')\n",
    "    # display(df.convert_dtypes(convert_string=True,convert_integer=True))\n",
    "    #display(df.dtypes)\n",
    "    # pd.DataFrame(df.isna().sum(),columns=['column_name','null_count'])\n",
    "    # null_ds = pd.DataFrame(df.isna().sum(),columns=['Null_Count'])\n",
    "    # nan_ds =df.size()['nan']\n",
    "    # nan_ds = pd.DataFrame(df.value_counts()['nan'],columns=['Nan_Count'])\n",
    "    # nan_ds = pd.DataFrame(df.isin(['nan','null','Nan', 'NaN', 'Null', ' ', '']).sum(),columns=['Nan&Null_Count']) \n",
    "    # display(df.describe(include='all'))\n",
    "    # Example 6: Get count duplicate rows\n",
    "    # number_duplication = len(df)-len(df.drop_duplicates())\n",
    "    # dict_d = {'col1': number_duplication}\n",
    "    # df2 = pd.DataFrame(data=dict_d,index=[\"count_dup\"])\n",
    "    # display(df2)\n",
    "    # nan_ds = pd.DataFrame(df.value_counts(),columns=['Nan_Count'])\n",
    "    e.to_csv (f'{f}_value_counts.csv', \n",
    "                    index = True,\n",
    "                    header=True)\n",
    "\n",
    "\n",
    "    # # value_counts = pd.DataFrame(np.unique(df.values, return_counts=True))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use glob to get all the csv files \n",
    "# in the folder\n",
    "path = os.getcwd()\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "#xlsx location\n",
    "xlsx_files = glob.glob(os.path.join(path, \"*.xlsx\"))\n",
    "\n",
    "# loop over the list of csv files\n",
    "e = pd.DataFrame()\n",
    "for f in csv_files:\n",
    "      \n",
    "    # read the csv file\n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "    e = pd.concat([e,df])\n",
    "\n",
    "    File_name =  f.split(\"\\\\\")[-1]\n",
    "# d = []\n",
    "# counter = 0\n",
    "for (columnName, columnData) in e.items():\n",
    "    for i in range(len(columnName)):\n",
    "    # e.get('columnName')\n",
    "    # print(e.iloc[:,0])\n",
    "    # print(e.iloc[:,counter])\n",
    "    # print(e.iloc[:,counter].isin(columnData).all())\n",
    "        # if columnName == e.iloc[:,i].name:\n",
    "        #     print(columnName,e.iloc[:,i].name)\n",
    "        if columnData.isin(e.iloc[:,i]).all()  == True and columnData.dropna().empty == False and columnName == 'LOT_BATCH':\n",
    "            print(columnName,e.iloc[:,i].name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c20fde216d2155112c50961c43343037d5dc4859966493adc092aeb7349463d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
