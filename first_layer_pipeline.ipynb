{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc05563e-1a74-4165-b19e-19a79f3a60f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read Me\n",
    "### This notebook created to list and read files from parameterized S3 location and file names and save into ingestion layer (1st Layer DBFS location/Schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbc2f149-6448-4264-a63f-9fe367d83bf9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d7ef14-8009-495e-9c48-0e9567cabd4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "import logging\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "509476c0-968d-42bf-b90e-a61b4b508de7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Parameterize storage location and file names to read for DRY principle and define common variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df563bd-3f1e-4772-95d5-f4b2d3963bc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_location = dbutils.widgets.dropdown(\"cloud_location\", \"merkle-de-interview-case-study\", [\"merkle-de-interview-case-study\"])\n",
    "\n",
    "storage_location_name = dbutils.widgets.get(\"cloud_location\")\n",
    "\n",
    "##Get notebook name as schema_name in case to be used in namespaces\n",
    "schema_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get().split(\"/\")[-1]\n",
    "\n",
    "## Metastore_name\n",
    "location_name = \"hive_metastore\"\n",
    "\n",
    "## Creating a logger to facilitate further debug activities\n",
    "logger_name = \"first_layer_pipeline_logger\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cefbe08-ecc5-43af-9144-db8794a0a9d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Setting Down Logger for Debug Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca491a5-cbbd-49cc-b102-788c4cb48548",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning Logger\nERROR:root:Error Issue Logger\nCRITICAL:root:Critical Issue Logger\n"
     ]
    }
   ],
   "source": [
    "logging.debug(\"Debug record and not logged\")\n",
    "\n",
    "logging.info(\"Info record and not logged\")\n",
    "\n",
    "logging.warning(\"Warning Logger\")\n",
    "\n",
    "logging.error(\"Error Issue Logger\")\n",
    "\n",
    "logging.critical(\"Critical Issue Logger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c958fda6-d2a3-41c0-81ba-543f0d9b7dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(f'{logger_name}_Data_Logger')\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "formatter=logging.Formatter('%(asctime)s-%(name)s-%(levelname)s-%(funcName)s:%(message)s',datefmt='%d/%m/%Y %I:%M:%S%P')\n",
    "\n",
    "logFileHandler = logging.FileHandler(f'/dbfs/FileStore/{logger_name}_logfile',mode='w')\n",
    "\n",
    "logFileHandler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(logFileHandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8f39d1-b64e-471c-9d1e-10fbbe07556b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Defining file names in storage location and dataframe lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd51cad-aa1d-4e50-9e68-8eb31570e3fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['item_df', 'event_df']\n{'item.csv': 'item_df', 'event.csv': 'event_df'}\n"
     ]
    }
   ],
   "source": [
    "# If access provided to S3 location I could save file names save in a list for a loop based read operation\n",
    "\n",
    "# file_list_schema = StructType([\n",
    "# StructField('path',StringType()),\n",
    "# StructField('name',StringType()),\n",
    "# StructField('size',IntegerType())\n",
    "# ])\n",
    "\n",
    "# file_list = dbutils.fs.ls(f\"s3a://{bucket_name}/de/\")\n",
    "# file_list_df = spark.createDataFrame(file_list,file_list_schema)\n",
    "\n",
    "# file_list = file_list_df.select('name').rdd.map(lambda x : x[0]).collect()\n",
    "\n",
    "## Created file list as hard coded due to access unavailability to storage address\n",
    "file_list = [\"item.csv\",\"event.csv\"]\n",
    "\n",
    "#Creating dataframe names dynamically\n",
    "data_frame_names = []\n",
    "for file_name in file_list:\n",
    "    data_frame_names.append(file_name.split(\".\")[0]+\"_df\")\n",
    "    \n",
    "print(data_frame_names)\n",
    "\n",
    "#Creating dict for file names and dataframe names\n",
    "df_file_dict = {}\n",
    "for file_name in file_list:\n",
    "    for data_frame in data_frame_names:\n",
    "        df_file_dict[file_name] = data_frame\n",
    "        data_frame_names.remove(data_frame)\n",
    "        break\n",
    "\n",
    "print(df_file_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4487924f-830e-4881-967d-da077749ef8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Defining a function for CSV extension file reads from storage location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad24843f-b4e4-4a2c-a222-e28a428bccf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_file(storage_location_name,dataframe_name,file_list = []):\n",
    "    \"\"\"\n",
    "    Creates dataframe based on defined elements data_frame_names list.\n",
    "    Reads files from cloud storage location conditionally by the file type \n",
    "\n",
    "    Args:\n",
    "        dataframe_name (str): The name of the dataframe.\n",
    "        storage_location_name (str): The name of the storage location name.\n",
    "        file_list (list): Name of the files to be read from storage location saved in a list.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: True if the parameters exists, False otherwise.\n",
    "    \"\"\"\n",
    "    for file_name in file_list:\n",
    "        if \"csv\" in file_name:\n",
    "            dataframe_name = (spark.read.format(\"csv\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .option(\"quote\",'\"')\\\n",
    "            .option(\"escape\",'\"')\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"delimiter\", \",\")\\\n",
    "            .load(f\"s3a://{storage_location_name}/de/{file_name}\"))\n",
    "        else:\n",
    "            print(f\"No CSV files exists in {storage_location_name} cloud storage location\")\n",
    "    return dataframe_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e345fa-dd09-432f-8e41-7f995875b887",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Defining Function to write Dataframes into schema by loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b842ed-f63c-47e3-a4be-335e65a0a653",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_schema(schema_name,location_name):\n",
    "    \"\"\"\n",
    "    Checks if a schema exists in the spark catalog.\n",
    "\n",
    "    Args:\n",
    "        location_name (str): catalog name to save schema on\n",
    "        schema_name (str): The name of the schema to check.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Empty DF creates schema on defined catalog if not exits\n",
    "    \"\"\"\n",
    "    return spark.sql(f\"CREATE SCHEMA IF NOT EXISTS  {location_name}.{schema_name}\")\n",
    "\n",
    "\n",
    "def check_if_table_exists(schema_name, table_name):\n",
    "    \"\"\"\n",
    "    Checks if a table exists in the spark catalog.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): The name of the table to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the table exists, False otherwise.\n",
    "    \"\"\"\n",
    "    return spark.catalog.tableExists(f\"hive_metastore.{schema_name}.{table_name}_bronze_layer_managed_table\")\n",
    "\n",
    "def write_to_managed_table(df, table_name, schema_name, location_name, mode = \"overwrite\"):\n",
    "    \"\"\"\n",
    "    Writes a DataFrame to a managed table in Delta Lake.\n",
    "\n",
    "    If the table exists and mode is overwrite, it performs an overwrite operation.\n",
    "    Otherwise, it either creates a new table or appends transactions to table based on the `mode` parameter.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): The DataFrame to write to the table.\n",
    "        table_name (str): The name of the target table.\n",
    "        schema_name (str): The schema name of target table.\n",
    "        location_name (str): Catalog name to save schema on\n",
    "        mode (str, optional): The write mode.\n",
    "    \"\"\"\n",
    "    #create schema if not exists\n",
    "    create_schema(schema_name,location_name)\n",
    "    # check if the table exists\n",
    "    if check_if_table_exists(schema_name, table_name):\n",
    "        print(f\"Table exists on hive_metastore.{schema_name}.{table_name}_bronze_layer_managed_table\")\n",
    "        if mode == \"overwrite\":\n",
    "            print(f\"Overwriting all transactions on managed table hive_metastore.{schema_name}.{table_name}_bronze_layer_managed_table\")\n",
    "            df.write.format(\"delta\").option(\"delta.columnMapping.mode\", \"name\").mode(mode).saveAsTable(f\"hive_metastore.{schema_name}.{table_name}_bronze_layer_managed_table\")\n",
    "        else:\n",
    "            print(f\"Appending all transactions on managed table hive_metastore.{schema_name}.{table_name}_bronze_layer_managed_table\")\n",
    "            df.write.format(\"delta\").option(\"delta.columnMapping.mode\", \"name\").mode(mode).saveAsTable(f\"hive_metastore.{schema_name}.{table_name}_bronze_layer_managed_table\")\n",
    "    else:\n",
    "        print(f\"Writing to managed table hive_metastore.{schema_name}.{table_name}_bronze_layer_managed_table\")\n",
    "        df.write.format(\"delta\").option(\"delta.columnMapping.mode\", \"name\").saveAsTable(f\"hive_metastore.{schema_name}.{table_name}_bronze_layer_managed_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436af9f2-9c45-40d4-961b-4ed3c0b06781",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reading tables from s3 bucket and logging details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9408c4ec-7ed7-493c-b5f4-17bc08f879c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:first_layer_pipeline_logger_Data_Logger:First_layer_pipeline data from S3 buckets read started\n"
     ]
    }
   ],
   "source": [
    "message = '{} data from S3 buckets read started'.format(f'{\"First_layer_pipeline\"}')\n",
    "\n",
    "logger.info(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29e72f28-765a-45c0-8bae-d57dec30f5aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pipeline_first():\n",
    "    for key,value in df_file_dict.items():\n",
    "        print(key,value)\n",
    "        # Reading csv files and assigning to defined values from dict\n",
    "        vars()[value] = read_csv_file(storage_location_name,value,[key])\n",
    "        # Casting all columns to string datatype\n",
    "        vars()[value] = vars()[value].select([col(f\"`{c}`\").cast(\"string\") for c in vars()[value].columns])\n",
    "        # Writing dataframes to 1st layer on defined schema and table\n",
    "        write_to_managed_table(vars()[value], value, schema_name, location_name)\n",
    "        # Logging size of the dataframes saved to 1st layer\n",
    "        shape_df = (vars()[value].count(),len(vars()[value].columns))\n",
    "        message = '{} files read from s3 bucket completed. Total {} rows & columns loaded into dataframe for {} first layer schema write'.format(f'{\"First_layer_pipeline\"}',shape_df,value)\n",
    "        logger.info(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc959071-454d-45f7-83cf-c2d892279b55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item.csv item_df\nTable exists on hive_metastore.first_layer_pipeline.item_df_bronze_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.first_layer_pipeline.item_df_bronze_layer_managed_table\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:first_layer_pipeline_logger_Data_Logger:First_layer_pipeline files read from s3 bucket completed. Total (2198, 7) rows & columns loaded into dataframe for item_df first layer schema write\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event.csv event_df\nTable exists on hive_metastore.first_layer_pipeline.event_df_bronze_layer_managed_table\nOverwriting all transactions on managed table hive_metastore.first_layer_pipeline.event_df_bronze_layer_managed_table\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:first_layer_pipeline_logger_Data_Logger:First_layer_pipeline files read from s3 bucket completed. Total (853640, 4) rows & columns loaded into dataframe for event_df first layer schema write\n"
     ]
    }
   ],
   "source": [
    "pipeline_first()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 762249934805174,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "first_layer_pipeline",
   "widgets": {
    "cloud_location": {
     "currentValue": "merkle-de-interview-case-study",
     "nuid": "c38d4541-bf26-4564-be7d-23037fde4986",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "merkle-de-interview-case-study",
      "label": null,
      "name": "cloud_location",
      "options": {
       "widgetType": "dropdown",
       "choices": [
        "merkle-de-interview-case-study"
       ]
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
